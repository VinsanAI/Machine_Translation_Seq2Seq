{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine Translation Seq2Seq.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UCvlKRxMWVe6","colab_type":"text"},"source":["# Neural machine translation - Encoder-Decoder seq2seq model\n","\n","## Machine Translation – A Brief History\n","Most of us were introduced to machine translation when Google came up with the service. But the concept has been around since the middle of last century.\n","\n","Research work in Machine Translation (MT) started as early as 1950’s, primarily in the United States. These early systems relied on huge bilingual dictionaries, hand-coded rules, and universal principles underlying natural language.\n","\n","In 1954, IBM held a first ever public demonstration of a machine translation. The system had a pretty small vocabulary of only 250 words and it could translate only 49 hand-picked Russian sentences to English. The number seems minuscule now but the system is widely regarded as an important milestone in the progress of machine translation.\n","\n","Soon, two schools of thought emerged:\n","\n","- Empirical trial-and-error approaches, using statistical methods, and\n","- Theoretical approaches involving fundamental linguistic research\n","\n","In 1964, the Automatic Language Processing Advisory Committee (ALPAC) was established by the United States government to evaluate the progress in Machine Translation. ALPAC did a little prodding around and published a report in November 1966 on the state of MT. Below are the key highlights from that report:\n","\n","- It raised serious questions on the feasibility of machine translation and termed it hopeless\n","- Funding was discouraged for MT research\n","- It was quite a depressing report for the researchers working in this field\n","- Most of them left the field and started new careers\n","\n","Not exactly a glowing recommendation!\n","\n","A long dry period followed this miserable report. Finally, in 1981, a new system called the METEO System was deployed in Canada for translation of weather forecasts issued in French into English. It was quite a successful project which stayed in operation until 2001.\n","\n","The world’s first web translation tool, Babel Fish, was launched by the AltaVista search engine in 1997.\n","\n","And then came the breakthrough we are all familiar with now – Google Translate. It has since changed the way we work (and even learn) with different languages."]},{"cell_type":"markdown","metadata":{"id":"TZfhXnODWhxD","colab_type":"text"},"source":["### Introduction to Sequence-to-Sequence (Seq2Seq) Modeling\n","Sequence-to-Sequence (seq2seq) models are used for a variety of NLP tasks, such as text summarization, speech recognition, DNA sequence modeling, among others. Our aim is to translate given sentences from one language to another.\n","\n","Here, both the input and output are sentences. In other words, these sentences are a sequence of words going in and out of a model. This is the basic idea of Sequence-to-Sequence modeling. \n","\n","A typical seq2seq model has 2 major components –\n","\n","- an encoder\n","- a decoder\n","\n","Both these parts are essentially two different recurrent neural network (RNN) models combined into one giant network\n","\n","Use cases of Sequence-to-Sequence modeling below (apart from Machine Translation, of course):\n","\n","- Speech Recognition\n","- Name Entity/Subject Extraction to identify the main subject from a body of text\n","- Relation Classification to tag relationships between various entities tagged in the above step\n","- Chatbot skills to have conversational ability and engage with customers\n","- Text Summarization to generate a concise summary of a large amount of text\n","- Question Answering systems\n"," "]},{"cell_type":"markdown","metadata":{"id":"qsaTi3nzWlTK","colab_type":"text"},"source":["### Business example\n","\n","For our example implementation, we will use a dataset of pairs of English sentences and their French translation, which you can download from manythings.org/anki. The file to download is called fra-eng.zip. \n","\n","We will implement a character-level sequence-to-sequence model, processing the input character-by-character and generating the output character-by-character. Another option would be a word-level model, which tends to be more common for machine translation. \n","\n","#### Here's a summary of our process:\n","\n","- 1) Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n","    - encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n","    - decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n","    - decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n","- 2) Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. Our model uses teacher forcing.\n","- 3) Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data).\n","Because the training process and inference process (decoding sentences) are quite different, we use different models for both, albeit they all leverage the same inner layers.\n","\n","This is our training model. It leverages three key features of Keras RNNs:\n","\n","- The return_state contructor argument, configuring a RNN layer to return a list where the first entry is the outputs and the next entries are the internal RNN states. This is used to recover the states of the encoder.\n","- The inital_state call argument, specifying the initial state(s) of a RNN. This is used to pass the encoder states to the decoder as initial states.\n","- The return_sequences constructor argument, configuring a RNN to return its full sequence of outputs (instead of just the last output, which the defaults behavior). This is used in the decoder."]},{"cell_type":"markdown","metadata":{"id":"7rMGvBn4WguV","colab_type":"text"},"source":["**Summary of the algorithm**\n","- We start with input sequences from a domain (e.g. English sentences)\n","    and corresponding target sequences from another domain\n","    (e.g. French sentences).\n","- An encoder LSTM turns input sequences to 2 state vectors\n","    (we keep the last LSTM state and discard the outputs).\n","- A decoder LSTM is trained to turn the target sequences into\n","    the same sequence but offset by one timestep in the future,\n","    a training process called \"teacher forcing\" in this context.\n","    It uses as initial state the state vectors from the encoder.\n","    Effectively, the decoder learns to generate `targets[t+1...]`\n","    given `targets[...t]`, conditioned on the input sequence.\n","- In inference mode, when we want to decode unknown input sequences, we:\n","    - Encode the input sequence into state vectors\n","    - Start with a target sequence of size 1\n","        (just the start-of-sequence character)\n","    - Feed the state vectors and 1-char target sequence\n","        to the decoder to produce predictions for the next character\n","    - Sample the next character using these predictions\n","        (we simply use argmax).\n","    - Append the sampled character to the target sequence\n","    - Repeat until we generate the end-of-sequence character or we\n","        hit the character limit."]},{"cell_type":"code","metadata":{"id":"NQL8K8qKW3O7","colab_type":"code","outputId":"10f543a1-db18-4a81-c278-9460de008f5f","executionInfo":{"status":"ok","timestamp":1572239386415,"user_tz":-330,"elapsed":26313,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZXC0knDfW4dF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":79},"outputId":"53d39bdc-7ee9-47d2-8e8c-748323c4005a","executionInfo":{"status":"ok","timestamp":1572239388287,"user_tz":-330,"elapsed":28173,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}}},"source":["# Importing Packages\n","\n","strFilePath = \"/content/drive/My Drive/GitUpload/Machine Translation Seq2Seq/fra.txt\"\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"IVv1QNdFW4p-","colab_type":"code","outputId":"1bbedeb2-fdcf-4ec5-a0a7-1e539471155e","executionInfo":{"status":"ok","timestamp":1572239390102,"user_tz":-330,"elapsed":29967,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Importing Data\n","fileInput = open(strFilePath, encoding=\"utf-8\").read().split('\\n')\n","fileInput[:2]"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\ufeffGo.\\tVa !', 'Run!\\tCours\\u202f!']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"xDLeIfdcYVTX","colab_type":"code","colab":{}},"source":["# Creating empty lists \n","\n","listEnglishInput = []\n","listFrenchInput = []\n","setEnglishChars = set()\n","setFrenchChars = set()\n","intSamples = 10000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQaQiVgjZ0y_","colab_type":"code","colab":{}},"source":["for index in range(intSamples):\n","\n","  # CurrentString = str(fileInput[index]).encode('ascii','ignore')\n","  # CurrentString = CurrentString.decode('utf-8')\n","  CurrentString = str(fileInput[index])\n","\n","  strEngLine = CurrentString.split('\\t')[0]\n","  # strEngLine = strEngLine.encode('ascii','ignore')\n","  strFreLine = '\\t' + CurrentString.split('\\t')[1] + '\\n'\n","  # strFreLine = strFreLine.encode('ascii','ignore')\n","\n","  listEnglishInput.append(strEngLine)\n","  listFrenchInput.append(strFreLine)\n","\n","  for ch in strEngLine:\n","    if (ch not in setEnglishChars):\n","      setEnglishChars.add(ch)\n","\n","  for ch in strFreLine:\n","    if (ch not in setFrenchChars):\n","      setFrenchChars.add(ch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtDkteBSboJf","colab_type":"code","outputId":"77267b31-a278-4639-ee85-a12e1538a880","executionInfo":{"status":"ok","timestamp":1572239390110,"user_tz":-330,"elapsed":29942,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(listEnglishInput[:3])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["['\\ufeffGo.', 'Run!', 'Run!']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4OElC9Derwcw","colab_type":"code","outputId":"75b52eb0-1be3-4c95-d1d4-73f0a2d9cd08","executionInfo":{"status":"ok","timestamp":1572239390111,"user_tz":-330,"elapsed":29923,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(listFrenchInput[:3])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["['\\tVa !\\n', '\\tCours\\u202f!\\n', '\\tCourez\\u202f!\\n']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FuZ31IyFbwRt","colab_type":"code","outputId":"894fa0ed-55ea-4455-97df-238431fff776","executionInfo":{"status":"ok","timestamp":1572239390112,"user_tz":-330,"elapsed":29905,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["setEnglishChars"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{' ',\n"," '!',\n"," '$',\n"," '&',\n"," \"'\",\n"," ',',\n"," '-',\n"," '.',\n"," '0',\n"," '1',\n"," '2',\n"," '3',\n"," '4',\n"," '5',\n"," '6',\n"," '7',\n"," '9',\n"," ':',\n"," '?',\n"," 'A',\n"," 'B',\n"," 'C',\n"," 'D',\n"," 'E',\n"," 'F',\n"," 'G',\n"," 'H',\n"," 'I',\n"," 'J',\n"," 'K',\n"," 'L',\n"," 'M',\n"," 'N',\n"," 'O',\n"," 'P',\n"," 'Q',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'U',\n"," 'V',\n"," 'W',\n"," 'Y',\n"," 'Z',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z',\n"," '’',\n"," '\\ufeff'}"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"3zATouTScHn2","colab_type":"code","outputId":"c5ab3560-bb7f-4d22-e9e9-89e321b68ed7","executionInfo":{"status":"ok","timestamp":1572239390113,"user_tz":-330,"elapsed":29887,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["setFrenchChars"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'\\t',\n"," '\\n',\n"," ' ',\n"," '!',\n"," '$',\n"," '&',\n"," \"'\",\n"," '(',\n"," ')',\n"," ',',\n"," '-',\n"," '.',\n"," '0',\n"," '1',\n"," '5',\n"," '6',\n"," '9',\n"," ':',\n"," '?',\n"," 'A',\n"," 'B',\n"," 'C',\n"," 'D',\n"," 'E',\n"," 'F',\n"," 'G',\n"," 'H',\n"," 'I',\n"," 'J',\n"," 'K',\n"," 'L',\n"," 'M',\n"," 'N',\n"," 'O',\n"," 'P',\n"," 'Q',\n"," 'R',\n"," 'S',\n"," 'T',\n"," 'U',\n"," 'V',\n"," 'Y',\n"," 'Z',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z',\n"," '\\xa0',\n"," '«',\n"," '»',\n"," 'À',\n"," 'Ç',\n"," 'É',\n"," 'Ê',\n"," 'à',\n"," 'â',\n"," 'ç',\n"," 'è',\n"," 'é',\n"," 'ê',\n"," 'ë',\n"," 'î',\n"," 'ï',\n"," 'ô',\n"," 'ù',\n"," 'û',\n"," 'œ',\n"," '\\u2009',\n"," '‘',\n"," '’',\n"," '\\u202f'}"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"dJoUN5t1gnEs","colab_type":"code","colab":{}},"source":["setFrenchChars = sorted(list(setFrenchChars))\n","setEnglishChars = sorted(list(setEnglishChars))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMHHUtDBg2nW","colab_type":"code","outputId":"48cdfdde-b90a-478e-e2ef-15ef7bc24742","executionInfo":{"status":"ok","timestamp":1572239390115,"user_tz":-330,"elapsed":29865,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["eng_index_to_char = {}\n","eng_char_to_index = {}\n","\n","for k,v in enumerate(setEnglishChars):\n","  eng_index_to_char[k] = v\n","  eng_char_to_index[v] = k\n","\n","eng_char_to_index"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{' ': 0,\n"," '!': 1,\n"," '$': 2,\n"," '&': 3,\n"," \"'\": 4,\n"," ',': 5,\n"," '-': 6,\n"," '.': 7,\n"," '0': 8,\n"," '1': 9,\n"," '2': 10,\n"," '3': 11,\n"," '4': 12,\n"," '5': 13,\n"," '6': 14,\n"," '7': 15,\n"," '9': 16,\n"," ':': 17,\n"," '?': 18,\n"," 'A': 19,\n"," 'B': 20,\n"," 'C': 21,\n"," 'D': 22,\n"," 'E': 23,\n"," 'F': 24,\n"," 'G': 25,\n"," 'H': 26,\n"," 'I': 27,\n"," 'J': 28,\n"," 'K': 29,\n"," 'L': 30,\n"," 'M': 31,\n"," 'N': 32,\n"," 'O': 33,\n"," 'P': 34,\n"," 'Q': 35,\n"," 'R': 36,\n"," 'S': 37,\n"," 'T': 38,\n"," 'U': 39,\n"," 'V': 40,\n"," 'W': 41,\n"," 'Y': 42,\n"," 'Z': 43,\n"," 'a': 44,\n"," 'b': 45,\n"," 'c': 46,\n"," 'd': 47,\n"," 'e': 48,\n"," 'f': 49,\n"," 'g': 50,\n"," 'h': 51,\n"," 'i': 52,\n"," 'j': 53,\n"," 'k': 54,\n"," 'l': 55,\n"," 'm': 56,\n"," 'n': 57,\n"," 'o': 58,\n"," 'p': 59,\n"," 'q': 60,\n"," 'r': 61,\n"," 's': 62,\n"," 't': 63,\n"," 'u': 64,\n"," 'v': 65,\n"," 'w': 66,\n"," 'x': 67,\n"," 'y': 68,\n"," 'z': 69,\n"," '’': 70,\n"," '\\ufeff': 71}"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"idIXHKiLg2uI","colab_type":"code","outputId":"5accf86a-69af-497b-af77-1bbfd6ee5195","executionInfo":{"status":"ok","timestamp":1572239390116,"user_tz":-330,"elapsed":29847,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["fre_index_to_char = {}\n","fre_char_to_index = {}\n","\n","for k,v in enumerate(setFrenchChars):\n","  fre_index_to_char[k] = v\n","  fre_char_to_index[v] = k\n","\n","fre_char_to_index"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'\\t': 0,\n"," '\\n': 1,\n"," ' ': 2,\n"," '!': 3,\n"," '$': 4,\n"," '&': 5,\n"," \"'\": 6,\n"," '(': 7,\n"," ')': 8,\n"," ',': 9,\n"," '-': 10,\n"," '.': 11,\n"," '0': 12,\n"," '1': 13,\n"," '5': 14,\n"," '6': 15,\n"," '9': 16,\n"," ':': 17,\n"," '?': 18,\n"," 'A': 19,\n"," 'B': 20,\n"," 'C': 21,\n"," 'D': 22,\n"," 'E': 23,\n"," 'F': 24,\n"," 'G': 25,\n"," 'H': 26,\n"," 'I': 27,\n"," 'J': 28,\n"," 'K': 29,\n"," 'L': 30,\n"," 'M': 31,\n"," 'N': 32,\n"," 'O': 33,\n"," 'P': 34,\n"," 'Q': 35,\n"," 'R': 36,\n"," 'S': 37,\n"," 'T': 38,\n"," 'U': 39,\n"," 'V': 40,\n"," 'Y': 41,\n"," 'Z': 42,\n"," 'a': 43,\n"," 'b': 44,\n"," 'c': 45,\n"," 'd': 46,\n"," 'e': 47,\n"," 'f': 48,\n"," 'g': 49,\n"," 'h': 50,\n"," 'i': 51,\n"," 'j': 52,\n"," 'k': 53,\n"," 'l': 54,\n"," 'm': 55,\n"," 'n': 56,\n"," 'o': 57,\n"," 'p': 58,\n"," 'q': 59,\n"," 'r': 60,\n"," 's': 61,\n"," 't': 62,\n"," 'u': 63,\n"," 'v': 64,\n"," 'w': 65,\n"," 'x': 66,\n"," 'y': 67,\n"," 'z': 68,\n"," '\\xa0': 69,\n"," '«': 70,\n"," '»': 71,\n"," 'À': 72,\n"," 'Ç': 73,\n"," 'É': 74,\n"," 'Ê': 75,\n"," 'à': 76,\n"," 'â': 77,\n"," 'ç': 78,\n"," 'è': 79,\n"," 'é': 80,\n"," 'ê': 81,\n"," 'ë': 82,\n"," 'î': 83,\n"," 'ï': 84,\n"," 'ô': 85,\n"," 'ù': 86,\n"," 'û': 87,\n"," 'œ': 88,\n"," '\\u2009': 89,\n"," '‘': 90,\n"," '’': 91,\n"," '\\u202f': 92}"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"xD8sPrwfulf-","colab_type":"code","outputId":"ac3eae59-0a0b-4a93-cdef-a8e17de1573e","executionInfo":{"status":"ok","timestamp":1572239390117,"user_tz":-330,"elapsed":29823,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["max_len_eng_sent = max([len(line) for line in listEnglishInput])\n","max_len_fre_sent = max([len(line) for line in listFrenchInput])\n","\n","print(max_len_eng_sent)\n","print(max_len_fre_sent)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["16\n","59\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zZsfBaynulM5","colab_type":"code","outputId":"17c9a876-0dc1-4e1c-940c-451ff7d7d864","executionInfo":{"status":"ok","timestamp":1572239390118,"user_tz":-330,"elapsed":29804,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(len(setEnglishChars))\n","print(len(setFrenchChars))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["72\n","93\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lQKof1Cgueky","colab_type":"code","colab":{}},"source":["tokenized_eng_sent = np.zeros(shape=(intSamples, max_len_eng_sent, len(setEnglishChars)), dtype='float32')\n","tokenized_fre_sent = np.zeros(shape=(intSamples, max_len_fre_sent, len(setFrenchChars)), dtype='float32')\n","target_data = np.zeros(shape=(intSamples, max_len_fre_sent, len(setFrenchChars)), dtype='float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUw8FisAM4rT","colab_type":"code","outputId":"c7469cd4-f997-4859-c9d8-aaaf3fbe0d64","executionInfo":{"status":"ok","timestamp":1572239390120,"user_tz":-330,"elapsed":29781,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["print(tokenized_eng_sent.shape)\n","print(tokenized_fre_sent.shape)\n","print(target_data.shape)\n","\n","#(Observation_num, Positions, Character)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["(10000, 16, 72)\n","(10000, 59, 93)\n","(10000, 59, 93)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M9DRqMK0PpJL","colab_type":"code","colab":{}},"source":["for i in range(intSamples):\n","  for pos,ch in enumerate(listEnglishInput[i]):\n","    tokenized_eng_sent[i,pos,eng_char_to_index[ch]] = 1\n","\n","  for pos,ch in enumerate(listFrenchInput[i]):\n","    tokenized_fre_sent[i,pos,fre_char_to_index[ch]] = 1\n","\n","    if pos>0:\n","      target_data[i,pos-1,fre_char_to_index[ch]] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlIREp8AR6UO","colab_type":"code","outputId":"a6cb9806-201c-407f-dfc5-8865d2c9d740","executionInfo":{"status":"ok","timestamp":1572239390122,"user_tz":-330,"elapsed":29758,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":840}},"source":["target_data"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 1., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       ...,\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 1., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 1., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 1., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"hwYA6CIMM46c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":138},"outputId":"a1c33ca4-fd51-4c6c-ef5c-b237820671ec","executionInfo":{"status":"ok","timestamp":1572239390584,"user_tz":-330,"elapsed":30212,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}}},"source":["encoder_input = Input(shape=(None, len(setEnglishChars)))\n","encoder_LSTM = LSTM(256, return_state=True)\n","encoder_output, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n","encoder_states = [encoder_h, encoder_c]"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W-y2lEpcSzRh","colab_type":"code","colab":{}},"source":["decoder_input = Input(shape=(None, len(setFrenchChars)))\n","decoder_LSTM = LSTM(256, return_sequences=True, return_state=True)\n","decoder_out, _, _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n","decoder_dense = Dense(len(setFrenchChars), activation='softmax')\n","decoder_out = decoder_dense(decoder_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXRGYZqKTk7x","colab_type":"code","outputId":"2142cc1b-7d9f-46fc-f579-c5f9768c82dd","executionInfo":{"status":"ok","timestamp":1572239806192,"user_tz":-330,"elapsed":335763,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":406}},"source":["model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_out)\n","\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(x=[tokenized_eng_sent, tokenized_fre_sent],\n","          y=target_data,\n","          batch_size=64,\n","          epochs=5,\n","          validation_split=0.2)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Train on 8000 samples, validate on 2000 samples\n","Epoch 1/5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","8000/8000 [==============================] - 69s 9ms/step - loss: 0.9380 - acc: 0.0639 - val_loss: 1.0059 - val_acc: 0.1181\n","Epoch 2/5\n","8000/8000 [==============================] - 66s 8ms/step - loss: 0.7627 - acc: 0.1111 - val_loss: 0.8056 - val_acc: 0.1478\n","Epoch 3/5\n","8000/8000 [==============================] - 66s 8ms/step - loss: 0.6414 - acc: 0.1374 - val_loss: 0.7276 - val_acc: 0.1618\n","Epoch 4/5\n","8000/8000 [==============================] - 66s 8ms/step - loss: 0.5814 - acc: 0.1525 - val_loss: 0.6990 - val_acc: 0.1705\n","Epoch 5/5\n","8000/8000 [==============================] - 66s 8ms/step - loss: 0.5403 - acc: 0.1645 - val_loss: 0.6401 - val_acc: 0.1868\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fde28d1ecf8>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"7alSCdzczWtr","colab_type":"code","outputId":"ad6cb9af-5feb-428b-e820-1e2303f07672","executionInfo":{"status":"ok","timestamp":1572207315184,"user_tz":-330,"elapsed":934289,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":554}},"source":["model.fit(x=[tokenized_eng_sent, tokenized_fre_sent],\n","          y=target_data,\n","          batch_size=64,\n","          epochs=15,\n","          validation_split=0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 8000 samples, validate on 2000 samples\n","Epoch 1/15\n","8000/8000 [==============================] - 63s 8ms/step - loss: 0.5017 - val_loss: 0.6130\n","Epoch 2/15\n","8000/8000 [==============================] - 63s 8ms/step - loss: 0.4747 - val_loss: 0.5912\n","Epoch 3/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.4508 - val_loss: 0.5682\n","Epoch 4/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.4305 - val_loss: 0.5583\n","Epoch 5/15\n","8000/8000 [==============================] - 63s 8ms/step - loss: 0.4126 - val_loss: 0.5426\n","Epoch 6/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3968 - val_loss: 0.5314\n","Epoch 7/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3823 - val_loss: 0.5209\n","Epoch 8/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3689 - val_loss: 0.5133\n","Epoch 9/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3565 - val_loss: 0.5092\n","Epoch 10/15\n","8000/8000 [==============================] - 63s 8ms/step - loss: 0.3443 - val_loss: 0.4986\n","Epoch 11/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3330 - val_loss: 0.4950\n","Epoch 12/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3222 - val_loss: 0.4872\n","Epoch 13/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3120 - val_loss: 0.4861\n","Epoch 14/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.3024 - val_loss: 0.4877\n","Epoch 15/15\n","8000/8000 [==============================] - 62s 8ms/step - loss: 0.2930 - val_loss: 0.4846\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7ff345ad9c50>"]},"metadata":{"tags":[]},"execution_count":142}]},{"cell_type":"code","metadata":{"id":"26W3hQ7QUbIm","colab_type":"code","colab":{}},"source":["# Building inference models\n","\n","# Encoder Inference model\n","encoder_model_inf = Model(encoder_input, encoder_states)\n","\n","# Decoder Inference Model\n","decoder_states_input_h = Input(shape=(256,))\n","decoder_states_input_c = Input(shape=(256,))\n","decoder_input_states = [decoder_states_input_h, decoder_states_input_c]\n","\n","decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\n","decoder_states = [decoder_h, decoder_c]\n","\n","decoder_out = decoder_dense(decoder_out)\n","decoder_model_inf = Model(inputs=[decoder_input]+decoder_input_states, outputs=[decoder_out]+decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CpShhHWfbtNq","colab_type":"code","colab":{}},"source":["def decode_seq(inp_seq):\n","  states_val = encoder_model_inf.predict(inp_seq) # extracting state values of my encoder\n","\n","  target_seq = np.zeros((1,1,len(setFrenchChars)))\n","  target_seq[0,0,fre_char_to_index['\\t']] = 1  # We are initializing our target prediction with <SOS> in this case its \\t\n","\n","  translated_sent = ''\n","  stop_condition = False\n","\n","  while not stop_condition:\n","    decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n","    max_val_index = np.argmax(decoder_out[0,-1,:])\n","    sampled_fra_char = fre_index_to_char[max_val_index]\n","    translated_sent += sampled_fra_char\n","\n","    if (sampled_fra_char == '\\n') or (len(translated_sent)>max_len_fre_sent):\n","      stop_condition = True\n","    \n","    target_seq = np.zeros((1, 1, len(setFrenchChars)))\n","    target_seq[0, 0, max_val_index] = 1\n","\n","    states_val = [decoder_h, decoder_c]\n","\n","  return translated_sent   \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnaF_vxIuOeB","colab_type":"code","outputId":"65c43031-a3ec-443b-e224-f059dc7d5c34","executionInfo":{"status":"ok","timestamp":1572207315789,"user_tz":-330,"elapsed":932847,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["(decode_seq(tokenized_eng_sent[0:1]))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Va chercher !\\n'"]},"metadata":{"tags":[]},"execution_count":145}]},{"cell_type":"code","metadata":{"id":"s_Lyb___tgcY","colab_type":"code","outputId":"0ac251fc-502c-464b-96ba-74f94cec1001","executionInfo":{"status":"ok","timestamp":1572207315792,"user_tz":-330,"elapsed":931936,"user":{"displayName":"Santhosh Bhm","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZn-Igy0r8bgRmRw-ea7dLmUGmsEAk1z3xj4GWwg=s64","userId":"07862723697335680693"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for seq_index in range(10):\n","  inp_seq = tokenized_eng_sent[seq_index:seq_index+1]\n","  translated_sent = decode_seq(inp_seq)\n","  print('-')\n","  print('Input sentence is : ',listEnglishInput[seq_index])\n","  print('Translated sentence is : ',translated_sent)\n","  print('Actual translated sentence is : ',listFrenchInput[seq_index])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-\n","Input sentence is :  ﻿Go.\n","Translated sentence is :  Va chercher !\n","\n","Actual translated sentence is :  \tVa !\n","\n","-\n","Input sentence is :  Run!\n","Translated sentence is :  Pardez-vous !\n","\n","Actual translated sentence is :  \tCours !\n","\n","-\n","Input sentence is :  Run!\n","Translated sentence is :  Pardez-vous !\n","\n","Actual translated sentence is :  \tCourez !\n","\n","-\n","Input sentence is :  Wow!\n","Translated sentence is :  Attrape les !\n","\n","Actual translated sentence is :  \tÇa alors !\n","\n","-\n","Input sentence is :  Fire!\n","Translated sentence is :  Arrête !\n","\n","Actual translated sentence is :  \tAu feu !\n","\n","-\n","Input sentence is :  Help!\n","Translated sentence is :  Prends un me chanten.\n","\n","Actual translated sentence is :  \tÀ l'aide !\n","\n","-\n","Input sentence is :  Jump.\n","Translated sentence is :  Sourlez !\n","\n","Actual translated sentence is :  \tSaute.\n","\n","-\n","Input sentence is :  Stop!\n","Translated sentence is :  Arrête !\n","\n","Actual translated sentence is :  \tÇa suffit !\n","\n","-\n","Input sentence is :  Stop!\n","Translated sentence is :  Arrête !\n","\n","Actual translated sentence is :  \tStop !\n","\n","-\n","Input sentence is :  Stop!\n","Translated sentence is :  Arrête !\n","\n","Actual translated sentence is :  \tArrête-toi !\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wme7XnIkzZIe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}